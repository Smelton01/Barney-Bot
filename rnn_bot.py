# -*- coding: utf-8 -*-
"""Barney_bot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yud4Aj2GapZGxr8tPQ55sRfnATdhRxDG
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

import numpy as np
from collections import Counter
import os
import re

# clone project

train_file='lines.txt'
seq_size=12
batch_size=16
embedding_size=64
lstm_size=64
gradients_norm=5
initial_words=["Eventually", "we"]
predict_top_k=5
checkpoint_path='~/checkpoint'

def pre_text():
    """Input text file"""

    with open("lines.txt", 'r', encoding="utf-8") as f:
        text = f.readlines()

    """Clean text to remove extra crap"""

    clean_text = [re.sub(r"^.*:", '', line).strip() for line in text]

    # Remove the clutter from your text
    for i, line in enumerate(clean_text):
        if "annotation" in line:
            clean_text.remove(line)
        if "}\"" in line:
            lin = line.split()
            lin.remove("}\"")
            clean_text[i] = " ".join(lin)
        for word in line:
            if "referent" in word:
                clean_text.remove(line)

    # Remove scene and mood descriptions (in brackets)
    crap = re.compile(r"\(.*\)|\[.*\]|\*.*\*|^\*.*$|^.*\*$|^song.*$|^-.*$")
    text_data = [re.sub(crap, '', line).strip() for line in clean_text if re.sub(crap, '', line)]
    with open("clean_text.txt", "w", encoding="utf-8") as f:
        for l in text_data:
            f.write(l+" ")

    #Vocabulary

    data = " ".join(text_data)
    data = data.split()
    word_counts = Counter(data)
    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)

    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}
    vocab_to_int = {w: k for k, w in int_to_vocab.items()}
    n_vocab = len(int_to_vocab)

    int_text = [vocab_to_int[w] for w in data]
    num_batches = int(len(int_text) / (seq_size * batch_size))
    in_text = int_text[:num_batches * batch_size * seq_size]
    out_text = np.zeros_like(in_text)
    out_text[:-1] = in_text[1:]
    out_text[-1] = in_text[0]
    in_text = np.reshape(in_text, (batch_size, -1))
    out_text = np.reshape(out_text, (batch_size, -1))
    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text

def get_batches(in_text, out_text, batch_size, seq_size):
    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)
    for i in range(0, num_batches * seq_size, seq_size):
        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]

#Model

class RNNModule(nn.Module):
    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):
        super(RNNModule, self).__init__()
        self.seq_size = seq_size
        self.lstm_size = lstm_size
        self.embedding = nn.Embedding(n_vocab, embedding_size)
        self.lstm = nn.LSTM(embedding_size,
                            lstm_size,
                            batch_first=True)
        self.dense = nn.Linear(lstm_size, n_vocab)

    def forward(self, x, prev_state):
        embed = self.embedding(x)
        output, state = self.lstm(embed, prev_state)
        logits = self.dense(output)

        return logits, state
    def zero_state(self, batch_size):
        return (torch.zeros(1, batch_size, self.lstm_size),
                torch.zeros(1, batch_size, self.lstm_size))

#Loss

def get_loss_and_train_op(net, lr=0.001):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)

    return criterion, optimizer

def run(epoch):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    #int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(
    #    flags.train_file, flags.batch_size, flags.seq_size)
    net = RNNModule(n_vocab, seq_size,
                    embedding_size, lstm_size)
    net = net.to(device)
    criterion, optimizer = get_loss_and_train_op(net, 0.01)
    iteration = 0
    
    for e in range(epoch):
        batches = get_batches(in_text, out_text, batch_size, seq_size)
        state_h, state_c = net.zero_state(batch_size)
        
        # Transfer data to GPU
        state_h = state_h.to(device)
        state_c = state_c.to(device)
        for x, y in batches:
            iteration += 1
            
            # Tell it we are in training mode
            net.train()

            # Reset all gradients
            optimizer.zero_grad()

            # Transfer data to GPU
            x = torch.tensor(x).to(device)
            y = torch.tensor(y).to(device)

            logits, (state_h, state_c) = net(x, (state_h, state_c))
            loss = criterion(logits.transpose(1, 2), y)

            state_h = state_h.detach()
            state_c = state_c.detach()

            loss_value = loss.item()

            # Perform back-propagation
            loss.backward()
            _ = torch.nn.utils.clip_grad_norm_(
                  net.parameters(), gradients_norm)
            # Update the network's parameters
            optimizer.step()

            if iteration % 100 == 0:
              print('Epoch: {}/{}'.format(e, epoch),
                    'Iteration: {}'.format(iteration),
                    'Loss: {}'.format(loss_value))

            if iteration % 1000 == 0:
              predict(device, net, initial_words, n_vocab,
                      vocab_to_int, int_to_vocab, top_k=5)
              torch.save(net.state_dict(),
                      './checkpoint/model-{}.pth'.format(iteration))

#inference"
"""

def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):
    net.eval()

    state_h, state_c = net.zero_state(1)
    state_h = state_h.to(device)
    state_c = state_c.to(device)
    for w in words:
        ix = torch.tensor([[vocab_to_int[w]]]).to(device)
        output, (state_h, state_c) = net(ix, (state_h, state_c))
    
    _, top_ix = torch.topk(output[0], k=top_k)
    choices = top_ix.tolist()
    choice = np.random.choice(choices[0])

    words.append(int_to_vocab[choice])
    
    for _ in range(100):
        ix = torch.tensor([[choice]]).to(device)
        output, (state_h, state_c) = net(ix, (state_h, state_c))

        _, top_ix = torch.topk(output[0], k=top_k)
        choices = top_ix.tolist()
        choice = np.random.choice(choices[0])
        words.append(int_to_vocab[choice])

    print(' '.join(words))

#run(10)

"""
def predict2(words, device, net, n_vocab, vocab_to_int, int_to_vocab, top_k=5):
    net.eval()
    state_h, state_c = net.zero_state(1)
    state_h = state_h.to(device)
    state_c = state_c.to(device)
    for w in words:
        ix = torch.tensor([[vocab_to_int[w]]]).to(device)
        output, (state_h, state_c) = net(ix, (state_h, state_c))
    
    _, top_ix = torch.topk(output[0], k=top_k)
    choices = top_ix.tolist()
    choice = np.random.choice(choices[0])

    words.append(int_to_vocab[choice])
    
    for _ in range(100):
        ix = torch.tensor([[choice]]).to(device)
        output, (state_h, state_c) = net(ix, (state_h, state_c))

        _, top_ix = torch.topk(output[0], k=top_k)
        choices = top_ix.tolist()
        choice = np.random.choice(choices[0])
        words.append(int_to_vocab[choice])

    return ' '.join(words)

def pred(prefix):
    int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = pre_text()
    model = RNNModule(n_vocab, seq_size,
                    embedding_size, lstm_size)
    print("Model done!!")
    model.load_state_dict(torch.load("./model-26000.pth", map_location="cpu"))
    dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(dev)
    print("predicting....")
    return predict2(prefix, device=dev, net=model, n_vocab=n_vocab, vocab_to_int=vocab_to_int, int_to_vocab=int_to_vocab)
    #print("DONE")
